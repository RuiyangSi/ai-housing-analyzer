#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼š
1. å¤„ç†æ‰€æœ‰çœå¸‚çš„CSVæ•°æ®
2. æŒ‰çœä»½ç»„ç»‡æ•°æ®ï¼ˆä¸€ä¸ªçœçš„ä¸åŒå¸‚åˆå¹¶ï¼‰
3. ç­›é€‰2023-2025å¹´çš„æ•°æ®
4. ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„è¾“å‡º
"""

import pandas as pd
import os
import json
from datetime import datetime

def clean_unit_price(price_str):
    """æ¸…ç†å•ä»·å­—ç¬¦ä¸²"""
    if pd.isna(price_str):
        return None
    if isinstance(price_str, (int, float)):
        return price_str
    price_str = str(price_str).replace('å…ƒ', '').replace(',', '').replace('*', '').strip()
    try:
        return float(price_str)
    except:
        return None

def clean_total_price(price_str):
    """æ¸…ç†æ€»ä»·å­—ç¬¦ä¸²"""
    if pd.isna(price_str):
        return None
    if isinstance(price_str, (int, float)):
        return price_str
    price_str = str(price_str).replace('*', '').strip()
    try:
        return float(price_str)
    except:
        return None

def clean_data(df, province_name):
    """
    æ•°æ®æ¸…æ´—å‡½æ•°
    1. ä¸€è‡´æ€§æ ¡éªŒï¼šæˆäº¤ä»· â‰ˆ å•ä»· Ã— é¢ç§¯ / 10000
    2. å¼‚å¸¸å€¼è¿‡æ»¤ï¼šåˆ é™¤æç«¯ä»·æ ¼
    3. å»é‡å¤„ç†ï¼šåˆ é™¤é‡å¤è®°å½•
    """
    original_len = len(df)
    print(f"     ğŸ”§ å¼€å§‹æ•°æ®æ¸…æ´—...")
    
    # 1. ä¸€è‡´æ€§æ ¡éªŒï¼šè®¡ç®—é¢„æœŸæˆäº¤ä»·ï¼Œæ£€æŸ¥è¯¯å·®
    # é¢„æœŸæˆäº¤ä»· = å•ä»· Ã— é¢ç§¯ / 10000 (è½¬æ¢ä¸ºä¸‡å…ƒ)
    df['é¢„æœŸæˆäº¤ä»·'] = df['æˆäº¤å•ä»·ï¼ˆå…ƒï¼‰'] * df['é¢ç§¯ï¼ˆmÂ²ï¼‰'] / 10000
    
    # è®¡ç®—è¯¯å·®æ¯”ä¾‹ï¼š|å®é™… - é¢„æœŸ| / é¢„æœŸ
    df['ä»·æ ¼è¯¯å·®'] = abs(df['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'] - df['é¢„æœŸæˆäº¤ä»·']) / df['é¢„æœŸæˆäº¤ä»·']
    
    # åˆ é™¤è¯¯å·®è¶…è¿‡50%çš„è®°å½•ï¼ˆä»·æ ¼ä¸ä¸€è‡´ï¼‰
    before_consistency = len(df)
    df = df[df['ä»·æ ¼è¯¯å·®'] <= 0.5]
    consistency_removed = before_consistency - len(df)
    if consistency_removed > 0:
        print(f"     âš ï¸  ä¸€è‡´æ€§æ ¡éªŒ: åˆ é™¤ {consistency_removed:,} æ¡ä»·æ ¼ä¸ä¸€è‡´è®°å½•")
    
    # 2. å¼‚å¸¸å€¼è¿‡æ»¤
    # æˆäº¤ä»·èŒƒå›´ï¼š10ä¸‡ - 5000ä¸‡ï¼ˆåˆç†çš„ä½å®…ä»·æ ¼èŒƒå›´ï¼‰
    before_outlier = len(df)
    df = df[(df['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'] >= 10) & (df['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'] <= 5000)]
    outlier_removed = before_outlier - len(df)
    if outlier_removed > 0:
        print(f"     âš ï¸  å¼‚å¸¸å€¼è¿‡æ»¤: åˆ é™¤ {outlier_removed:,} æ¡æç«¯ä»·æ ¼è®°å½•")
    
    # å•ä»·èŒƒå›´ï¼š1000å…ƒ/ã¡ - 300000å…ƒ/ã¡
    before_unit_outlier = len(df)
    df = df[(df['æˆäº¤å•ä»·ï¼ˆå…ƒï¼‰'] >= 1000) & (df['æˆäº¤å•ä»·ï¼ˆå…ƒï¼‰'] <= 300000)]
    unit_outlier_removed = before_unit_outlier - len(df)
    if unit_outlier_removed > 0:
        print(f"     âš ï¸  å•ä»·å¼‚å¸¸è¿‡æ»¤: åˆ é™¤ {unit_outlier_removed:,} æ¡å•ä»·å¼‚å¸¸è®°å½•")
    
    # é¢ç§¯èŒƒå›´ï¼š10ã¡ - 500ã¡
    before_area_outlier = len(df)
    df = df[(df['é¢ç§¯ï¼ˆmÂ²ï¼‰'] >= 10) & (df['é¢ç§¯ï¼ˆmÂ²ï¼‰'] <= 500)]
    area_outlier_removed = before_area_outlier - len(df)
    if area_outlier_removed > 0:
        print(f"     âš ï¸  é¢ç§¯å¼‚å¸¸è¿‡æ»¤: åˆ é™¤ {area_outlier_removed:,} æ¡é¢ç§¯å¼‚å¸¸è®°å½•")
    
    # 3. å»é‡å¤„ç†ï¼ˆåŸºäºå°åŒºã€æˆ·å‹ã€é¢ç§¯ã€æˆäº¤æ—¥æœŸã€æˆäº¤ä»·ï¼‰
    before_dedup = len(df)
    df = df.drop_duplicates(subset=['å°åŒº', 'æˆ·å‹', 'é¢ç§¯ï¼ˆmÂ²ï¼‰', 'æˆäº¤æ—¥æœŸ', 'æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'])
    dedup_removed = before_dedup - len(df)
    if dedup_removed > 0:
        print(f"     âš ï¸  å»é‡å¤„ç†: åˆ é™¤ {dedup_removed:,} æ¡é‡å¤è®°å½•")
    
    # åˆ é™¤ä¸´æ—¶åˆ—
    df = df.drop(columns=['é¢„æœŸæˆäº¤ä»·', 'ä»·æ ¼è¯¯å·®'])
    
    total_removed = original_len - len(df)
    print(f"     âœ… æ¸…æ´—å®Œæˆ: å…±åˆ é™¤ {total_removed:,} æ¡ ({total_removed/original_len*100:.1f}%), å‰©ä½™ {len(df):,} æ¡")
    
    return df

def process_all_data(data_dir='data/raw', output_dir='data/processed', start_year=2023, end_year=2025):
    """å¤„ç†æ‰€æœ‰åŸå¸‚æ•°æ®"""
    print("\n" + "="*80)
    print("å…¨å›½æˆ¿ä»·æ•°æ®ç»Ÿä¸€å¤„ç†å·¥å…·")
    print("="*80)
    
    # ç‰¹æ®Šæ–‡ä»¶åæ˜ å°„ï¼ˆæ–‡ä»¶å -> (çœä»½, åŸå¸‚)ï¼‰
    # æ ¹æ®åŸå§‹æ•°æ®URLä¸­çš„åŸå¸‚ä»£ç ç¡®å®šå®é™…åŸå¸‚
    special_mappings = {
        'anhui_deals': ('å®‰å¾½', 'åˆè‚¥å¸‚'),           # hf.esf.fang.com
        'hebei_all_deals_merged': ('æ²³åŒ—', 'ä¿å®šå¸‚'), # bd.esf.fang.com
        'heilongjiang_deals': ('é»‘é¾™æ±Ÿ', 'å“ˆå°”æ»¨å¸‚'), # hrb.esf.fang.com
        'jiangsu_deals': ('æ±Ÿè‹', 'æ— é”¡å¸‚'),         # wuxi.esf.fang.com
        'jilin_deals': ('å‰æ—', 'é•¿æ˜¥å¸‚'),           # changchun
        'liaoning_deals': ('è¾½å®', 'æ²ˆé˜³å¸‚'),        # sy
        'shanxi_deals': ('å±±è¥¿', 'å¤ªåŸå¸‚'),          # taiyuan.esf.fang.com (æ—§æ•°æ®)
        'shanxi_taiyuan_deals': ('å±±è¥¿', 'å¤ªåŸå¸‚'),  # æ–°å¢å¤ªåŸæ•°æ®
        'shanxi_datong_deals': ('å±±è¥¿', 'å¤§åŒå¸‚'),   # æ–°å¢å¤§åŒæ•°æ®
        'zhejiang_deals': ('æµ™æ±Ÿ', 'å®æ³¢å¸‚'),        # nb.esf.fang.com
        'å¦é—¨': ('ç¦å»º', 'å¦é—¨å¸‚'),                  # xm.esf.fang.com (æ–°å¢å¤§æ•°æ®é›†)
        'é‡åº†': ('é‡åº†', 'é‡åº†å¸‚'),                  # cq.esf.fang.com (ç›´è¾–å¸‚)
        'äº‘å—æ˜†æ˜': ('äº‘å—', 'æ˜†æ˜å¸‚'),              # æ–°å¢äº‘å—æ˜†æ˜
        'å››å·æˆéƒ½': ('å››å·', 'æˆéƒ½å¸‚'),              # æ–°å¢å››å·æˆéƒ½
        'å››å·ç»µé˜³': ('å››å·', 'ç»µé˜³å¸‚'),              # æ–°å¢å››å·ç»µé˜³
        'å¹¿ä¸œå¹¿å·': ('å¹¿ä¸œ', 'å¹¿å·å¸‚'),              # æ–°å¢å¹¿ä¸œå¹¿å·
        'å¹¿ä¸œæ·±åœ³': ('å¹¿ä¸œ', 'æ·±åœ³å¸‚'),              # æ–°å¢å¹¿ä¸œæ·±åœ³
        'è´µå·è´µé˜³': ('è´µå·', 'è´µé˜³å¸‚'),              # æ–°å¢è´µå·è´µé˜³
        'è´µå·éµä¹‰': ('è´µå·', 'éµä¹‰å¸‚'),              # æ–°å¢è´µå·éµä¹‰
    }
    
    # æ‰«ææ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
    print(f"\nğŸ“ å‘ç° {len(csv_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
    # æŒ‰çœä»½ç»„ç»‡æ–‡ä»¶
    province_cities = {}
    for filename in sorted(csv_files):
        file_key = filename.replace('.csv', '')
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šæ–‡ä»¶å
        if file_key in special_mappings:
            province, city = special_mappings[file_key]
        elif '-' in filename:
            # æ ¼å¼: "çœä»½-åŸå¸‚.csv"
            parts = filename.replace('.csv', '').split('-')
            province = parts[0]
            city = parts[1]
        else:
            # ç›´è¾–å¸‚æ ¼å¼: "åŸå¸‚.csv"
            city = filename.replace('.csv', '')
            if city in ['åŒ—äº¬å¸‚', 'ä¸Šæµ·å¸‚', 'å¤©æ´¥å¸‚', 'é‡åº†å¸‚']:
                province = city.replace('å¸‚', '')
            else:
                province = city.replace('å¸‚', '')
        
        if province not in province_cities:
            province_cities[province] = []
        province_cities[province].append({'filename': filename, 'city': city})
    
    print(f"\nğŸ—ºï¸  è¦†ç›–çœä»½: {len(province_cities)} ä¸ª")
    for province, cities in province_cities.items():
        city_names = ', '.join([c['city'] for c in cities])
        print(f"  â€¢ {province}: {city_names}")
    
    # å¤„ç†æ¯ä¸ªçœä»½çš„æ•°æ®
    all_results = []
    summary_stats = {}
    
    for province, cities in province_cities.items():
        print(f"\n{'='*80}")
        print(f"å¤„ç† {province} æ•°æ®...")
        print(f"{'='*80}")
        
        province_data = []
        province_total_count = 0
        
        for city_info in cities:
            filename = city_info['filename']
            city_name = city_info['city']
            file_path = os.path.join(data_dir, filename)
            
            print(f"\n  ğŸ“– è¯»å–: {filename}")
            
            try:
                # è¯»å–CSV
                df = pd.read_csv(file_path, low_memory=False)
                print(f"     åŸå§‹æ•°æ®: {len(df):,} æ¡")
                
                # è½¬æ¢æ—¥æœŸ
                df['æˆäº¤æ—¥æœŸ'] = pd.to_datetime(df['deal_date'], format='mixed', errors='coerce')
                
                # ç­›é€‰å¹´ä»½
                start_date = f'{start_year}-01-01'
                end_date = f'{end_year}-12-31'
                df_filtered = df[(df['æˆäº¤æ—¥æœŸ'] >= start_date) & (df['æˆäº¤æ—¥æœŸ'] <= end_date)]
                print(f"     {start_year}-{end_year}å¹´æ•°æ®: {len(df_filtered):,} æ¡")
                
                if len(df_filtered) == 0:
                    print(f"     âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰ {start_year}-{end_year} å¹´çš„æ•°æ®")
                    continue
                
                # æ•°æ®è½¬æ¢
                df_processed = pd.DataFrame()
                df_processed['æˆäº¤æ—¥æœŸ'] = df_filtered['æˆäº¤æ—¥æœŸ']
                df_processed['çœä»½'] = province
                df_processed['åŸå¸‚'] = city_name
                df_processed['åŒºåŸŸ'] = df_filtered['district']
                df_processed['å•†åœˆ'] = df_filtered['business_area']
                df_processed['å°åŒº'] = df_filtered['community']
                df_processed['æˆ·å‹'] = df_filtered['room_type']
                df_processed['é¢ç§¯ï¼ˆmÂ²ï¼‰'] = pd.to_numeric(df_filtered['area'], errors='coerce')
                df_processed['æŒ‚ç‰Œä»·ï¼ˆä¸‡å…ƒï¼‰'] = None
                df_processed['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'] = df_filtered['total_price'].apply(clean_total_price)
                df_processed['æˆäº¤å•ä»·ï¼ˆå…ƒï¼‰'] = df_filtered['unit_price'].apply(clean_unit_price)
                
                # æ¸…ç†ç¼ºå¤±å€¼
                original_len = len(df_processed)
                df_processed = df_processed.dropna(subset=['æˆäº¤æ—¥æœŸ', 'æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰', 'é¢ç§¯ï¼ˆmÂ²ï¼‰', 'æˆäº¤å•ä»·ï¼ˆå…ƒï¼‰'])
                removed = original_len - len(df_processed)
                if removed > 0:
                    print(f"     æ¸…ç†ç¼ºå¤±å€¼: åˆ é™¤äº† {removed} æ¡è®°å½•")
                
                if len(df_processed) > 0:
                    province_data.append(df_processed)
                    province_total_count += len(df_processed)
                    
                    # ç»Ÿè®¡
                    avg_price = df_processed['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'].mean()
                    avg_unit_price = df_processed['æˆäº¤å•ä»·ï¼ˆå…ƒï¼‰'].mean()
                    print(f"     âœ“ æœ‰æ•ˆæ•°æ®: {len(df_processed):,} æ¡")
                    print(f"     âœ“ å¹³å‡æˆäº¤ä»·: {avg_price:.2f} ä¸‡å…ƒ")
                    print(f"     âœ“ å¹³å‡å•ä»·: {avg_unit_price:.2f} å…ƒ/mÂ²")
                
            except Exception as e:
                print(f"     âŒ é”™è¯¯: {str(e)}")
                continue
        
        # åˆå¹¶çœä»½æ•°æ®
        if province_data:
            province_df = pd.concat(province_data, ignore_index=True)
            
            # æ•°æ®æ¸…æ´—
            province_df = clean_data(province_df, province)
            
            if len(province_df) == 0:
                print(f"     âš ï¸  è­¦å‘Šï¼š{province} æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®")
                continue
            
            # ä¿å­˜çœä»½æ•°æ®
            output_file = os.path.join(output_dir, f'data_{province}_2023_2025.csv')
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            province_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'province': province,
                'cities': [c['city'] for c in cities],
                'total_count': int(len(province_df)),
                'avg_price': round(float(province_df['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'].mean()), 2),
                'median_price': round(float(province_df['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'].median()), 2),
                'avg_unit_price': round(float(province_df['æˆäº¤å•ä»·ï¼ˆå…ƒï¼‰'].mean()), 2),
                'avg_area': round(float(province_df['é¢ç§¯ï¼ˆmÂ²ï¼‰'].mean()), 2),
                'min_price': round(float(province_df['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'].min()), 2),
                'max_price': round(float(province_df['æˆäº¤ä»·ï¼ˆä¸‡å…ƒï¼‰'].max()), 2)
            }
            summary_stats[province] = stats
            
            print(f"\n  âœ… {province} æ•°æ®å¤„ç†å®Œæˆ!")
            print(f"     æ€»æˆäº¤é‡: {len(province_df):,} å¥—")
            print(f"     å¹³å‡æˆäº¤ä»·: {stats['avg_price']:.2f} ä¸‡å…ƒ")
            print(f"     å¹³å‡å•ä»·: {stats['avg_unit_price']:.2f} å…ƒ/mÂ²")
            print(f"     ä¿å­˜åˆ°: {output_file}")
            
            all_results.append({
                'province': province,
                'count': len(province_df),
                'output_file': output_file
            })
    
    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    summary_file = os.path.join(output_dir, 'data_summary.json')
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_stats, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*80}")
    print("å¤„ç†å®Œæˆ!")
    print(f"{'='*80}")
    
    total_records = sum([r['count'] for r in all_results])
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  â€¢ å¤„ç†çœä»½: {len(all_results)} ä¸ª")
    print(f"  â€¢ æ€»æ•°æ®é‡: {total_records:,} æ¡")
    print(f"  â€¢ æ±‡æ€»æ–‡ä»¶: {summary_file}")
    
    print(f"\nğŸ“‹ å„çœæ•°æ®é‡:")
    for result in sorted(all_results, key=lambda x: x['count'], reverse=True):
        percentage = (result['count'] / total_records * 100) if total_records > 0 else 0
        print(f"  â€¢ {result['province']}: {result['count']:,} æ¡ ({percentage:.1f}%)")
    
    return all_results, summary_stats

if __name__ == '__main__':
    results, stats = process_all_data()
